diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index cd61d5b..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-/__pycache__
-/wandb
-/models
diff --git a/README.md b/README.md
deleted file mode 100644
index 6532921..0000000
--- a/README.md
+++ /dev/null
@@ -1,2 +0,0 @@
-# Intelligent-Motion-Control-Method-Based-on-Directional-Drive-for-3-DOF-Robotic-Arm
-Man, H., Ge, N., &amp; Xu, L. (2021, June). Intelligent Motion Control Method Based on Directional Drive for 3-DOF Robotic Arm. In 2021 5th International Conference on Robotics and Automation Sciences (ICRAS) (pp. 144-149). IEEE.
diff --git a/config.py b/config.py
index a6bfe0c..3c8271d 100644
--- a/config.py
+++ b/config.py
@@ -1,27 +1,36 @@
-#! /usr/bin/env python3
-# -*- coding: utf-8 -*-
+import torch
 
-cuda = "cuda:0"
-env_name = 'ned2'
-policy = "Gaussian"
+# 환경 설정
+ENV_NAME = 'Pendulum-v1'
+SEED = 0
 
-gamma = 0.99
-tau = 0.005
-lr = 0.001
-alpha = 0.2
+# 학습 설정
+EPISODES = 100000
+MAX_STEPS = 500
 
-seed = 123456
+# DDPG 하이퍼파라미터
+ACTOR_LR = 1e-4
+CRITIC_LR = 1e-3
+GAMMA = 0.99
+TAU = 0.001
+BATCH_SIZE = 64
 
-hidden_size = 64
-Success_Standard = 0.9
+# 리플레이 버퍼 설정
+BUFFER_SIZE = 100000
 
-num_steps = 10000001
-batch_size = 512
-start_steps = 10000
-max_episode_steps = 256
-time_sleep_interval = 0.05
+# 노이즈 설정
+NOISE_MU = 0
+NOISE_THETA = 0.15
+NOISE_SIGMA = 0.2
 
-isExit_IfSuccessLearning = True #목표 달성 시(success rate 0.9이상일 때) 학습을 종료할 것인지
+# 신경망 구조
+HIDDEN1_UNITS = 400
+HIDDEN2_UNITS = 300
 
-replay_size = num_steps #1000000
-cuda = "cuda"
+# 기기 설정
+DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+
+# 모델 저장 및 로드 설정
+SAVE_INTERVAL = 100
+ACTOR_PATH = 'actor.pth'
+CRITIC_PATH = 'critic.pth'
\ No newline at end of file
diff --git a/ddpg_agent.py b/ddpg_agent.py
index 2bf8d33..baac8ee 100644
--- a/ddpg_agent.py
+++ b/ddpg_agent.py
@@ -1,77 +1,75 @@
 import torch
 import torch.nn.functional as F
-import numpy as np
-from networks import Actor, Critic
+from actor import Actor
+from critic import Critic
+from replay_buffer import ReplayBuffer
+from noise import OUNoise
 import config
 
-
 class DDPGAgent:
-    def __init__(self, state_dim, action_dim, max_action, device):
-        self.actor = Actor(state_dim, action_dim, max_action).to(device)
-        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)
+    def __init__(self, state_dim, action_dim, max_action):
+        self.actor = Actor(state_dim, action_dim, max_action).to(config.DEVICE)
+        self.actor_target = Actor(state_dim, action_dim, max_action).to(config.DEVICE)
         self.actor_target.load_state_dict(self.actor.state_dict())
-        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())
+        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=config.ACTOR_LR)
 
-        self.critic = Critic(state_dim, action_dim).to(device)
-        self.critic_target = Critic(state_dim, action_dim).to(device)
+        self.critic = Critic(state_dim, action_dim).to(config.DEVICE)
+        self.critic_target = Critic(state_dim, action_dim).to(config.DEVICE)
         self.critic_target.load_state_dict(self.critic.state_dict())
-        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())
+        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=config.CRITIC_LR)
+
+        self.memory = ReplayBuffer(config.BUFFER_SIZE)
+        self.noise = OUNoise(action_dim, config.NOISE_MU, config.NOISE_THETA, config.NOISE_SIGMA)
 
         self.max_action = max_action
-        self.device = device
 
     def select_action(self, state):
-        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)
-        return self.actor(state).cpu().data.numpy().flatten()
-
-    def train(self, replay_buffer):
-        # Sample a batch from memory
-        state, action, next_state, reward, done = replay_buffer.sample(config.batch_size)
-        
-        state = state.to(self.device)
-        action = action.to(self.device)
-        next_state = next_state.to(self.device)
-        reward = reward.to(self.device)
-        done = done.to(self.device)
-
-        # Compute the target Q value
-        target_Q = self.critic_target(next_state, self.actor_target(next_state))
-        target_Q = reward + (done * config.gamma * target_Q).detach()
+        state = torch.FloatTensor(state.reshape(1, -1)).to(config.DEVICE)
+        action = self.actor(state).cpu().data.numpy().flatten()
+        return (action + self.noise.sample() * self.max_action).clip(-self.max_action, self.max_action)
 
-        # Get current Q estimate
-        current_Q = self.critic(state, action)
+    def store_transition(self, state, action, reward, next_state, done):
+        self.memory.push(state, action, reward, next_state, done)
+
+    def update(self):
+        if len(self.memory) < config.BATCH_SIZE:
+            return
+
+        state, action, reward, next_state, done = self.memory.sample(config.BATCH_SIZE)
 
-        # Compute critic loss
-        critic_loss = F.mse_loss(current_Q, target_Q)
+        state = torch.FloatTensor(state).to(config.DEVICE)
+        action = torch.FloatTensor(action).to(config.DEVICE)
+        reward = torch.FloatTensor(reward).reshape(-1, 1).to(config.DEVICE)
+        next_state = torch.FloatTensor(next_state).to(config.DEVICE)
+        done = torch.FloatTensor(done).reshape(-1, 1).to(config.DEVICE)
 
-        # Optimize the critic
+        # Update Critic
+        target_Q = self.critic_target(next_state, self.actor_target(next_state))
+        target_Q = reward + (1 - done) * config.GAMMA * target_Q
+        current_Q = self.critic(state, action)
+
+        critic_loss = F.mse_loss(current_Q, target_Q.detach())
         self.critic_optimizer.zero_grad()
         critic_loss.backward()
         self.critic_optimizer.step()
 
-        # Compute actor loss
+        # Update Actor
         actor_loss = -self.critic(state, self.actor(state)).mean()
-
-        # Optimize the actor
         self.actor_optimizer.zero_grad()
         actor_loss.backward()
         self.actor_optimizer.step()
 
-        # Update the frozen target models
+        # Update target networks
         for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
-            target_param.data.copy_(config.tau * param.data + (1 - config.tau) * target_param.data)
+            target_param.data.copy_(config.TAU * param.data + (1 - config.TAU) * target_param.data)
 
         for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
-            target_param.data.copy_(config.tau * param.data + (1 - config.tau) * target_param.data)
-
-    def save(self, filename):
-        torch.save(self.critic.state_dict(), filename + "_critic")
-        torch.save(self.critic_optimizer.state_dict(), filename + "_critic_optimizer")
-        torch.save(self.actor.state_dict(), filename + "_actor")
-        torch.save(self.actor_optimizer.state_dict(), filename + "_actor_optimizer")
-
-    def load(self, filename):
-        self.critic.load_state_dict(torch.load(filename + "_critic"))
-        self.critic_optimizer.load_state_dict(torch.load(filename + "_critic_optimizer"))
-        self.actor.load_state_dict(torch.load(filename + "_actor"))
-        self.actor_optimizer.load_state_dict(torch.load(filename + "_actor_optimizer"))
\ No newline at end of file
+            target_param.data.copy_(config.TAU * param.data + (1 - config.TAU) * target_param.data)
+
+    def save_models(self):
+        torch.save(self.actor.state_dict(), config.ACTOR_PATH)
+        torch.save(self.critic.state_dict(), config.CRITIC_PATH)
+
+    def load_models(self):
+        self.actor.load_state_dict(torch.load(config.ACTOR_PATH))
+        self.critic.load_state_dict(torch.load(config.CRITIC_PATH))
\ No newline at end of file
diff --git a/evaluate.py b/evaluate.py
deleted file mode 100644
index e487c02..0000000
--- a/evaluate.py
+++ /dev/null
@@ -1,36 +0,0 @@
-import numpy as np
-import torch
-import gym
-from ddpg_agent import DDPGAgent
-
-def evaluate():
-    env = gym.make("Pendulum-v1", render_mode="human")
-    state_dim = env.observation_space.shape[0]
-    action_dim = env.action_space.shape[0]
-    max_action = float(env.action_space.high[0])
-
-    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    agent = DDPGAgent(state_dim, action_dim, max_action, device)
-    
-    # Load the trained model
-    agent.load("ddpg_pendulum")
-
-    for episode in range(10):  # Evaluate for 10 episodes
-        state, _ = env.reset()
-        episode_reward = 0
-        done = False
-
-        while not done:
-            action = agent.select_action(np.array(state))
-            next_state, reward, terminated, truncated, _ = env.step(action)
-            done = terminated or truncated
-            
-            episode_reward += reward
-            state = next_state
-
-        print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}")
-
-    env.close()
-
-if __name__ == "__main__":
-    evaluate()
\ No newline at end of file
diff --git a/networks.py b/networks.py
deleted file mode 100644
index a86e01c..0000000
--- a/networks.py
+++ /dev/null
@@ -1,33 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import config
-
-class Actor(nn.Module):
-    def __init__(self, state_dim, action_dim, max_action):
-        super(Actor, self).__init__()
-        hidden_size = config.hidden_size
-
-        self.l1 = nn.Linear(state_dim, hidden_size)
-        self.l2 = nn.Linear(hidden_size, hidden_size)
-        self.l3 = nn.Linear(hidden_size, action_dim)
-        self.max_action = max_action
-
-    def forward(self, state):
-        a = F.relu(self.l1(state))
-        a = F.relu(self.l2(a))
-        return self.max_action * torch.tanh(self.l3(a))
-
-class Critic(nn.Module):
-    def __init__(self, state_dim, action_dim):
-        super(Critic, self).__init__()
-        hidden_size = config.hidden_size
-
-        self.l1 = nn.Linear(state_dim + action_dim, hidden_size)
-        self.l2 = nn.Linear(hidden_size, hidden_size)
-        self.l3 = nn.Linear(hidden_size, 1)
-
-    def forward(self, state, action):
-        q = F.relu(self.l1(torch.cat([state, action], 1)))
-        q = F.relu(self.l2(q))
-        return self.l3(q)
\ No newline at end of file
diff --git a/train.py b/train.py
deleted file mode 100644
index 147a447..0000000
--- a/train.py
+++ /dev/null
@@ -1,95 +0,0 @@
-import gym
-import numpy as np
-import torch
-from ddpg_agent import DDPGAgent
-from collections import deque
-import random
-import wandb
-import config
-
-class ReplayBuffer:
-    def __init__(self, max_size=config.replay_size):
-        self.buffer = deque(maxlen=int(max_size))
-
-    def add(self, state, action, reward, next_state, done):
-        self.buffer.append((state, action, reward, next_state, done))
-
-    def sample(self, batch_size):
-        batch = random.sample(self.buffer, batch_size)
-        state, action, reward, next_state, done = map(np.stack, zip(*batch))
-        return (
-            torch.FloatTensor(state),
-            torch.FloatTensor(action),
-            torch.FloatTensor(next_state),
-            torch.FloatTensor(reward).unsqueeze(1),
-            torch.FloatTensor(done).unsqueeze(1)
-        )
-
-    def __len__(self):
-        return len(self.buffer)
-
-def init_wandb():
-    wandb.init(project='DDPG_TEST')
-    wandb.run.name = 'Test'
-    wandb.run.save()
-
-def train():
-    init_wandb()
-    env = gym.make("Pendulum-v1")
-    state_dim = env.observation_space.shape[0]
-    action_dim = env.action_space.shape[0]
-    max_action = float(env.action_space.high[0])
-
-    device = torch.device(config.cuda if torch.cuda.is_available() else "cpu")
-    print(f"Using device: {device}")
-    
-    agent = DDPGAgent(state_dim, action_dim, max_action, device)
-    replay_buffer = ReplayBuffer()
-
-    total_timesteps = 0
-    episode_reward = 0
-    episode_timesteps = 0
-    episode_num = 0
-
-    state, _ = env.reset()
-
-    for t in range(config.num_steps):
-        total_timesteps += 1
-        episode_timesteps += 1
-
-        # Select action
-        if t < 10000:
-            action = env.action_space.sample()
-        else:
-            action = agent.select_action(np.array(state))
-            noise = np.random.normal(0, max_action * 0.1, size=action_dim)
-            action = (action + noise).clip(-max_action, max_action)
-
-        # Perform action
-        next_state, reward, terminated, truncated, _ = env.step(action)
-        done = terminated or truncated
-        
-        # Store data in replay buffer
-        replay_buffer.add(state, action, reward, next_state, done)
-
-        state = next_state 
-        episode_reward += reward
-
-        # Train agent
-        if len(replay_buffer) > 10000:
-            agent.train(replay_buffer)
-
-        if done:
-            print(f"Total T: {total_timesteps}, Episode Num: {episode_num}, Episode T: {episode_timesteps}, Reward: {episode_reward:.3f}")
-            wandb.log({"episode_reward": episode_reward})
-            # Reset environment
-            state, _ = env.reset()
-            episode_reward = 0
-            episode_timesteps = 0
-            episode_num += 1
-
-    # Save the trained model
-    agent.save("ddpg_pendulum")
-
-if __name__ == "__main__":
-    train()
\ No newline at end of file
