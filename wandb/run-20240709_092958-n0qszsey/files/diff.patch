diff --git a/Env.py b/Env.py
index c29f62f..604fad8 100644
--- a/Env.py
+++ b/Env.py
@@ -7,6 +7,8 @@ import moveit_msgs.msg
 import geometry_msgs.msg
 import math
 from moveit_commander.conversions import pose_to_list
+from moveit_msgs.msg import RobotState
+from tf.transformations import quaternion_matrix
 from gazebo_msgs.msg import ModelState 
 from gazebo_msgs.srv import SetModelState
 import config
@@ -22,9 +24,11 @@ class Ned2_control(object):
         rospy.init_node('move_group_python_interface', anonymous=True)
         group_name = "ned2" #moveit의 move_group name >> moveit assitant로 패키지 생성 시 정의
         move_group = moveit_commander.MoveGroupCommander(group_name) # move_group node로 동작을 계획하고,  실행 
-        
+        self.robot = moveit_commander.RobotCommander()
         self.move_group = move_group
 
+        print(self.robot)
+
         self.target = [0,0,0] #target 위치
 
         # action 관련
@@ -135,31 +139,38 @@ class Ned2_control(object):
         self.move_group.go([0,0,0,0,0,0], wait=True)
         self.set_random_target()
         
-    def get_initial_state(self): #joint 6축 각도
-        current_pose = self.move_group.get_current_joint_values()
-        distance = self.calc_distance(self.target, self.get_pose())
-        angle_difference = 0
-
-        state = current_pose + self.get_pose() + self.target + [distance] + [angle_difference]
-        if(len(state) == 14):
-            self.prev_state = state
-        else:
-            state = self.prev_state
-        return state
-
-    def get_state(self,current_pose, distance, angle_difference): #joint 6축 각도
-        state = current_pose + self.get_pose() + self.target + [distance] + [angle_difference]
-        if(len(state) == 14):
-            self.prev_state = state
-        else:
-            state = self.prev_state
-        return state
-
-    def get_pose(self):
+    def get_joint2_position(self):
+        joint3_pose = self.move_group.get_current_pose(end_effector_link="arm_link").pose
+        joint3_position = [joint3_pose.position.x, joint3_pose.position.y, joint3_pose.position.z]
+        return joint3_position
+
+    def get_joint3_position(self):
+        joint3_pose = self.move_group.get_current_pose(end_effector_link="elbow_link").pose
+        joint3_position = [joint3_pose.position.x, joint3_pose.position.y, joint3_pose.position.z]
+        return joint3_position
+    
+    def get_endeffector_position(self):
         pose = self.move_group.get_current_pose().pose
         pose_value = [pose.position.x,pose.position.y,pose.position.z]
         return pose_value
     
+    def get_state(self):
+        joint2_pos = self.get_joint2_position()
+        joint3_pos = self.get_joint3_position()
+        endeffector_pos = self.get_endeffector_position()
+        
+        relative_joint2 = [joint2_pos[0] - self.target[0], joint2_pos[1] - self.target[1], joint2_pos[2] - self.target[2]]
+        relative_joint3 = [joint3_pos[0] - self.target[0], joint3_pos[1] - self.target[1], joint3_pos[2] - self.target[2]]
+        relative_endeffector = [endeffector_pos[0] - self.target[0], endeffector_pos[1] - self.target[1], endeffector_pos[2] - self.target[2]]
+        
+        state = joint3_pos + endeffector_pos + relative_joint2 + relative_joint3 + relative_endeffector
+
+        if(len(state) == 15): self.prev_state = state
+        else : state = self.prev_state
+
+        return state
+
+
     def get_reward(self, distance, angle_difference):
         # R(position)
 
@@ -170,21 +181,22 @@ class Ned2_control(object):
         elif(11.25 > angle_difference >= 0): R_theta = 0.6
 
         # R(dinstance)
-        if(distance >= 1.0): R_distance = 0.0
-        elif(1.0 > distance >= 0.7): R_distance = 0.01
-        elif(1.0 > distance >= 0.5): R_distance = 0.06
-        elif(0.5 > distance >= 0.1): R_distance = 0.17
-        elif(0.1 > distance >= 0): R_distance = 1.17
+        df = 0.7
+        if(distance >= df): R_distance = 0.0
+        elif(df > distance >= df*0.7): R_distance = 0.01
+        elif(df*0.7 > distance >= df*0.5): R_distance = 0.06
+        elif(df*0.5 > distance >= df*0.1): R_distance = 0.17
+        elif(df*0.1 > distance >= 0): R_distance = 1.17
 
         isDone, isTruncated = False, False
-        if(self.time_step >= self.MAX_time_step) or (self.isLimited == True) or (self.get_pose()[2] < 0.1): isDone,isTruncated = False, True
+        if(self.time_step >= self.MAX_time_step) or (self.isLimited == True) or (self.get_endeffector_position()[2] < 0.1): isDone,isTruncated = False, True
         if(distance <= 0.03): isDone,isTruncated = True,False
 
         totalReward = R_theta + R_distance
         return totalReward, isDone,isTruncated
     
     def step(self, angle):
-        distance = self.calc_distance(self.target, self.get_pose())
+        distance = self.calc_distance(self.target, self.get_endeffector_position())
         # print(distance)
 
         df = 0.7
@@ -204,7 +216,7 @@ class Ned2_control(object):
         # print(f"Angle difference: {angle_difference} degrees")
 
         totalReward,isDone,isTruncated = self.get_reward(distance, angle_difference)
-        current_state = self.get_state(current_pose, distance, angle_difference)
+        current_state = 1
 
         return current_state,totalReward,isDone, isTruncated
     
@@ -242,4 +254,5 @@ if __name__ == "__main__":
     ned2_control.reset()
 
     while not rospy.is_shutdown():
-        print(ned2_control.step([0.0, -0.3, 0.35, 0, 0, 0]))
+        ned2_control.step([0.0, -0.3, 0.35, 0, 0, 0])
+        print(ned2_control.get_joint3_position())
diff --git a/__pycache__/Env.cpython-38.pyc b/__pycache__/Env.cpython-38.pyc
index bcf2662..dc619d8 100644
Binary files a/__pycache__/Env.cpython-38.pyc and b/__pycache__/Env.cpython-38.pyc differ
diff --git a/main.py b/main.py
index e140285..a13e5f8 100644
--- a/main.py
+++ b/main.py
@@ -16,7 +16,7 @@ def main():
     env = Env.Ned2_control()
     np.random.seed(config.SEED)
 
-    state_dim = 14
+    state_dim = 15
     action_dim = 3
     max_action = 1.0
 
@@ -24,7 +24,7 @@ def main():
     
     for episode in range(config.EPISODES):
         env.reset()  # OpenAI Gym의 새로운 버전에 맞춰 수정
-        state = env.get_initial_state()  # 새로운 env.get_state() 메소드 사용
+        state = env.get_state()  # 새로운 env.get_state() 메소드 사용
         episode_reward = 0
         
         for step in range(config.MAX_STEPS):
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index d4207f2..2c2e0de 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240708_221111-a6iu7vya/logs/debug-internal.log
\ No newline at end of file
+run-20240709_092958-n0qszsey/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 5473073..c7d14cb 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20240708_221111-a6iu7vya/logs/debug.log
\ No newline at end of file
+run-20240709_092958-n0qszsey/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index ef4e17a..1f44847 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20240708_221111-a6iu7vya
\ No newline at end of file
+run-20240709_092958-n0qszsey
\ No newline at end of file
